/** !!!! error result, under construction */

package com.lickov.scala

import org.apache.spark._   
import org.apache.spark.SparkContext



/** Code to extract data from S3 */

object Data_Extract_s3  {
  
  /** Makes sure only ERROR messages get logged to avoid log spam. */
  def setupLogging() = {
    import org.apache.log4j.{Level, Logger}   
    val rootLogger = Logger.getRootLogger()
    rootLogger.setLevel(Level.ERROR)   
  }
  
  // only for testing result !!!  Otherwise not needed.
  //Function to extract hero ID -> hero name tuples (or None in case of failure)
  def parseNames(line: String) : Option[(Int, String)] = {
    var fields = line.split('\"')
    if (fields.length > 1) {
      return Some(fields(0).trim().toInt, fields(1))
    } else {
      return None // flatmap will just discard None results, and extract data from Some results.
    }
  }
    
  
   def main(args: Array[String]) {
   
         // Create a SparkContext using every core of the local machine
        val sc = new SparkContext("local[*]", "Data_Extract_s3")
      	
        //AWS credentials as environment variables
        val accessKeyId = System.getenv("AWS_ACCESS_KEY_ID")
        val secretAccessKey = System.getenv("AWS_SECRET_ACCESS_KEY")
        sc.hadoopConfiguration.set("fs.s3n.awsAccessKeyId", accessKeyId)
        sc.hadoopConfiguration.set("fs.s3n.awsSecretAccessKey", secretAccessKey)
        
        val data = sc.textFile("s3n://playdata-lickov/Marvel-names.txt")
        
        //just as testing exercise 
				val namesRdd = data.flatMap(parseNames)

        
        
     /** TEST */
		 println(s"$namesRdd ")	    
   }
}
